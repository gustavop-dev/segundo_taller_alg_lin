\documentclass[12pt]{article}

%------------------------------------------------------------------------------
% Paquetes
%------------------------------------------------------------------------------
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

%------------------------------------------------------------------------------
% Datos del documento
%------------------------------------------------------------------------------
\title{Segundo Taller Computacional\\\vspace{0.3em}Álgebra Lineal Aplicada}
\author{Gustavo Adolfo Pérez Pérez\\Universidad Nacional de Colombia -- Sede Medellín\\Programa de Ciencias de la Computación}
\date{Fecha de entrega: 2 de julio de 2025}

%------------------------------------------------------------------------------
\begin{document}

\maketitle
\tableofcontents
\newpage

%------------------------------------------------------------------------------
% 1. Convergencia y estabilidad de sucesiones recurrentes
%------------------------------------------------------------------------------
\section{Convergencia y estabilidad de sucesiones recurrentes}

En esta sección analizamos el comportamiento asintótico del cociente $x_{n+1}/x_n$ para sucesiones definidas por recurrencias lineales homogéneas y su estabilidad ante perturbaciones en las condiciones iniciales.

\subsection{Marco teórico}

Consideremos una recurrencia lineal homogénea de orden $k$:
\[
x_n = a_1 x_{n-1} + a_2 x_{n-2} + \cdots + a_k x_{n-k}, \quad n \geq k
\]

con condiciones iniciales $x_0, x_1, \ldots, x_{k-1}$ dadas.

\subsubsection{Polinomio característico}

El comportamiento de la sucesión está determinado por las raíces del polinomio característico:
\[
p(\lambda) = \lambda^k - a_1 \lambda^{k-1} - a_2 \lambda^{k-2} - \cdots - a_k
\]

Si $\lambda_1, \lambda_2, \ldots, \lambda_k$ son las raíces (posiblemente complejas y con multiplicidad), la solución general es:
\[
x_n = \sum_{i=1}^{k} c_i \lambda_i^n
\]

donde los coeficientes $c_i$ se determinan por las condiciones iniciales resolviendo el sistema lineal:
\[
\begin{pmatrix}
1 & 1 & \cdots & 1 \\
\lambda_1 & \lambda_2 & \cdots & \lambda_k \\
\lambda_1^2 & \lambda_2^2 & \cdots & \lambda_k^2 \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_1^{k-1} & \lambda_2^{k-1} & \cdots & \lambda_k^{k-1}
\end{pmatrix}
\begin{pmatrix}
c_1 \\ c_2 \\ \vdots \\ c_k
\end{pmatrix}
=
\begin{pmatrix}
x_0 \\ x_1 \\ \vdots \\ x_{k-1}
\end{pmatrix}
\]

\subsubsection{Condiciones de convergencia}

\textbf{Teorema (Convergencia del cociente).} El límite
\[
L = \lim_{n \to \infty} \frac{x_{n+1}}{x_n}
\]
existe si y solo si:
\begin{enumerate}
    \item Existe una única raíz dominante $\lambda^*$ tal que $|\lambda^*| > |\lambda_i|$ para todo $i \neq *$
    \item El coeficiente $c^*$ correspondiente a $\lambda^*$ en la solución general es no nulo
\end{enumerate}

En tal caso, $L = \lambda^*$.

\textbf{Demostración.} Si se cumplen las condiciones, para $n$ grande:
\[
x_n = c^* (\lambda^*)^n + \sum_{i \neq *} c_i \lambda_i^n = c^* (\lambda^*)^n \left(1 + \sum_{i \neq *} \frac{c_i}{c^*} \left(\frac{\lambda_i}{\lambda^*}\right)^n\right)
\]

Como $|\lambda_i/\lambda^*| < 1$ para $i \neq *$, los términos adicionales tienden a cero:
\[
\frac{x_{n+1}}{x_n} = \lambda^* \cdot \frac{1 + O(r^n)}{1 + O(r^n)} \to \lambda^*
\]
donde $r = \max_{i \neq *} |\lambda_i/\lambda^*| < 1$. $\square$

\subsection{Implementación del algoritmo}

El algoritmo implementado sigue estos pasos:

\begin{enumerate}
    \item \textbf{Cálculo de raíces}: Encuentra las raíces del polinomio característico usando métodos numéricos robustos.
    
    \item \textbf{Identificación de raíces dominantes}: Determina las raíces con módulo máximo dentro de una tolerancia numérica.
    
    \item \textbf{Sistema de Vandermonde}: Resuelve el sistema lineal para obtener los coeficientes $c_i$.
    
    \item \textbf{Verificación de condiciones}: Comprueba que existe una única raíz dominante con coeficiente no nulo.
    
    \item \textbf{Método de respaldo}: Si el coeficiente dominante es numéricamente cero, realiza un análisis iterativo para detectar convergencia empírica.
\end{enumerate}

El código fuente completo está disponible en: \url{https://github.com/gustavop-dev/segundo_taller_alg_lin/blob/master/primer_punto/convergencia_sucesiones.py}

\subsection{Ejemplos de análisis}

\subsubsection{Ejemplo 1: Sucesión de Fibonacci}

La sucesión de Fibonacci se define por:
\[
x_n = x_{n-1} + x_{n-2}, \quad x_0 = 1, \, x_1 = 1
\]

El polinomio característico es $\lambda^2 - \lambda - 1 = 0$ con raíces:
\[
\lambda_1 = \frac{1 + \sqrt{5}}{2} \approx 1.618034, \quad \lambda_2 = \frac{1 - \sqrt{5}}{2} \approx -0.618034
\]

Como $|\lambda_1| > |\lambda_2|$, existe una raíz dominante única. El análisis computacional confirma:

\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Propiedad} & \textbf{Valor} \\
\hline
¿Converge? & Sí \\
Límite teórico & $\phi = \frac{1+\sqrt{5}}{2}$ \\
Límite numérico & 1.618033988749895 \\
Error absoluto & $< 10^{-15}$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Ejemplo 2: Sucesión de Lucas}

Consideremos la recurrencia $x_n = x_{n-1} + x_{n-2}$ con condiciones iniciales $x_0 = 2, x_1 = 1$:

\begin{itemize}
    \item Las raíces del polinomio característico son las mismas que Fibonacci
    \item Los coeficientes cambian: $c_1 = 1, c_2 = 1$
    \item Ambos coeficientes son no nulos, pero $\lambda_1$ sigue siendo dominante
    \item El cociente converge al mismo límite: $\phi$
\end{itemize}

\subsubsection{Ejemplo 3: Caso de no convergencia}

Para la recurrencia $x_n = -x_{n-2}$ con $x_0 = 1, x_1 = 0$:
\begin{itemize}
    \item Polinomio característico: $\lambda^2 + 1 = 0$
    \item Raíces: $\lambda_1 = i, \lambda_2 = -i$
    \item Ambas raíces tienen el mismo módulo: $|\lambda_1| = |\lambda_2| = 1$
    \item No existe raíz dominante única $\Rightarrow$ el cociente no converge
\end{itemize}

La sucesión oscila: $1, 0, -1, 0, 1, 0, -1, \ldots$ y el cociente no está definido en términos pares.

\subsection{Análisis de estabilidad}

\subsubsection{Estabilidad ante perturbaciones}

Analizamos cómo pequeñas perturbaciones en las condiciones iniciales afectan la convergencia.

Sea $\tilde{x}_0 = x_0 + \epsilon_0, \tilde{x}_1 = x_1 + \epsilon_1, \ldots$ las condiciones perturbadas. Los nuevos coeficientes satisfacen:
\[
\tilde{c} = c + V^{-1}\epsilon
\]

donde $V$ es la matriz de Vandermonde y $\epsilon = (\epsilon_0, \epsilon_1, \ldots, \epsilon_{k-1})^T$.

\textbf{Teorema (Estabilidad).} Si la raíz dominante $\lambda^*$ es simple y bien separada (i.e., $|\lambda^*|/|\lambda_i| > 1 + \delta$ para algún $\delta > 0$), entonces:
\begin{enumerate}
    \item El límite del cociente es estable: $\tilde{L} = L + O(\|\epsilon\|)$
    \item La convergencia se preserva para perturbaciones suficientemente pequeñas
\end{enumerate}

\subsubsection{Número de condición}

El número de condición de la matriz de Vandermonde $\kappa(V)$ determina la sensibilidad:
\[
\frac{\|\Delta c\|}{\|c\|} \leq \kappa(V) \frac{\|\epsilon\|}{\|x_{\text{inicial}}\|}
\]

Para raíces bien separadas, $\kappa(V) = O(1)$, pero para raíces cercanas puede crecer exponencialmente.

\subsection{Resultados computacionales}

\subsubsection{Experimentos numéricos}

Ejecutamos el algoritmo para diversas recurrencias:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Recurrencia} & \textbf{Coef. $a$} & \textbf{C.I.} & \textbf{¿Converge?} & \textbf{Límite} \\
\hline
Fibonacci & [1, 1] & [1, 1] & Sí & 1.6180 \\
Lucas & [1, 1] & [2, 1] & Sí & 1.6180 \\
Tribonacci & [1, 1, 1] & [1, 1, 1] & Sí & 1.8393 \\
Perrin & [0, 1, 1] & [3, 0, 2] & Sí & 1.3247 \\
Oscilante & [0, -1] & [1, 0] & No & -- \\
Degenerada & [2, -1] & [1, 2] & No & -- \\
\hline
\end{tabular}
\caption{Análisis de convergencia para diferentes sucesiones recurrentes}
\end{table}

\subsubsection{Análisis de sensibilidad}

Para la sucesión de Fibonacci, perturbamos las condiciones iniciales:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Perturbación $\epsilon$} & \textbf{C.I. perturbadas} & \textbf{Límite} & \textbf{Error relativo} \\
\hline
$10^{-10}$ & [1 + $10^{-10}$, 1] & 1.6180339887498951 & $6 \times 10^{-16}$ \\
$10^{-5}$ & [1 + $10^{-5}$, 1] & 1.6180339887498967 & $9 \times 10^{-15}$ \\
$10^{-3}$ & [1, 1 + $10^{-3}$] & 1.6180339887520813 & $1.3 \times 10^{-12}$ \\
$10^{-1}$ & [1.1, 1] & 1.6180339887498993 & $2.5 \times 10^{-14}$ \\
\hline
\end{tabular}
\caption{Estabilidad del límite ante perturbaciones en Fibonacci}
\end{table}

Los resultados confirman la excelente estabilidad cuando existe una raíz dominante bien separada.

\subsection{Visualización de la convergencia}

Para ilustrar la convergencia, graficamos la evolución del cociente $x_{n+1}/x_n$:

\begin{figure}[h]
\centering
% \includegraphics[width=0.8\textwidth]{convergencia_fibonacci.png}
\caption{Convergencia del cociente $x_{n+1}/x_n$ para Fibonacci. La línea punteada indica el valor límite $\phi$.}
\end{figure}

La convergencia es rápida y monótona, con error que decae exponencialmente como $O((\lambda_2/\lambda_1)^n) = O((-0.618)^n)$.

\subsection{Conclusiones}

\begin{enumerate}
    \item \textbf{Criterio de convergencia}: El cociente $x_{n+1}/x_n$ converge si y solo si existe una única raíz dominante del polinomio característico con coeficiente no nulo.
    
    \item \textbf{Valor límite}: Cuando converge, el límite es precisamente la raíz dominante $\lambda^*$.
    
    \item \textbf{Estabilidad}: Para raíces bien separadas, el límite es estable ante pequeñas perturbaciones en las condiciones iniciales.
    
    \item \textbf{Implementación robusta}: El algoritmo maneja casos degenerados mediante análisis iterativo cuando el coeficiente dominante es numéricamente cero. La implementación completa en Python está disponible en el repositorio del proyecto.
    
    \item \textbf{Aplicaciones}: Este análisis es fundamental en el estudio de algoritmos recursivos, análisis de complejidad y modelado de fenómenos de crecimiento.
\end{enumerate}

\subsection{Nota sobre la implementación}

El código desarrollado (\texttt{convergencia\_sucesiones.py}) proporciona una función \texttt{analiza\_convergencia} que:
\begin{itemize}
    \item Acepta coeficientes arbitrarios de la recurrencia
    \item Maneja condiciones iniciales generales
    \item Incluye tolerancia numérica configurable
    \item Implementa un método de respaldo para casos degenerados
    \item Retorna tanto el estado de convergencia como el valor límite
\end{itemize}

La implementación utiliza NumPy para cálculos numéricos eficientes y está documentada con docstrings detallados para facilitar su uso y comprensión.

\vspace{2cm}

%------------------------------------------------------------------------------
% 2. Estabilidad numérica de valores propios vs valores singulares
%------------------------------------------------------------------------------
\section{Estabilidad numérica de valores propios vs valores singulares}

En este problema analizaremos la estabilidad de los valores singulares frente a perturbaciones y la compararemos con la estabilidad de los valores propios.

\subsection{Estabilidad de los valores singulares}

\textbf{Teorema.} Sea $A \in \mathbb{C}^{m \times n}$ una matriz con valores singulares $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{\min(m,n)} \geq 0$. Si $E \in \mathbb{C}^{m \times n}$ es una perturbación tal que $\|E\|_2 = \varepsilon \ll 1$, entonces los valores singulares $\tilde{\sigma}_i$ de $A + E$ satisfacen:
\[
|\sigma_i - \tilde{\sigma}_i| \leq \varepsilon, \quad \text{para } i = 1, 2, \ldots, \min(m,n)
\]

\textbf{Demostración.} Utilizaremos el Teorema de Weyl para valores singulares. Sin pérdida de generalidad, supongamos $m \geq n$.

Primero, recordemos que los valores singulares de una matriz $M$ son las raíces cuadradas de los valores propios de $M^*M$. Para la matriz perturbada $A + E$, tenemos:
\[
(A + E)^*(A + E) = A^*A + A^*E + E^*A + E^*E
\]

Consideremos las matrices hermitianas aumentadas:
\[
H_A = \begin{pmatrix}
0 & A \\
A^* & 0
\end{pmatrix}, \quad
H_{A+E} = \begin{pmatrix}
0 & A + E \\
(A + E)^* & 0
\end{pmatrix}
\]

Los valores propios de $H_A$ son $\pm\sigma_1, \pm\sigma_2, \ldots, \pm\sigma_n$ (y ceros adicionales si $m > n$), donde $\sigma_i$ son los valores singulares de $A$.

Observemos que:
\[
H_{A+E} - H_A = \begin{pmatrix}
0 & E \\
E^* & 0
\end{pmatrix} = H_E
\]

La norma espectral de $H_E$ es:
\[
\|H_E\|_2 = \max_{i} |\lambda_i(H_E)| = \|E\|_2 = \varepsilon
\]

Por el Teorema de Weyl para valores propios de matrices hermitianas, si $\lambda_1 \geq \lambda_2 \geq \cdots$ son los valores propios de $H_A$ ordenados de forma decreciente, y $\tilde{\lambda}_1 \geq \tilde{\lambda}_2 \geq \cdots$ son los valores propios de $H_{A+E}$, entonces:
\[
|\lambda_i - \tilde{\lambda}_i| \leq \|H_E\|_2 = \varepsilon
\]

Como los valores singulares de $A$ y $A + E$ corresponden a los valores propios no negativos de $H_A$ y $H_{A+E}$ respectivamente, concluimos que:
\[
|\sigma_i - \tilde{\sigma}_i| \leq \varepsilon, \quad \text{para todo } i
\]

Esto completa la demostración. $\square$

\subsection{Inestabilidad de los valores propios}

A diferencia de los valores singulares, los valores propios pueden ser extremadamente sensibles a perturbaciones. Presentaremos dos ejemplos ilustrativos.

\textbf{Ejemplo 1: Matriz nilpotente.}
Consideremos la matriz nilpotente de orden $n$:
\[
A = \begin{pmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
0 & 0 & 0 & \cdots & 0
\end{pmatrix} \in \mathbb{R}^{n \times n}
\]

Esta matriz tiene todos sus valores propios iguales a cero: $\lambda_i(A) = 0$ para $i = 1, \ldots, n$.

Ahora consideremos la perturbación:
\[
E = \begin{pmatrix}
0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 0 & 0 \\
\delta & 0 & \cdots & 0 & 0
\end{pmatrix}
\]

donde $\delta > 0$ es pequeño. Entonces $\|E\|_2 = \delta$.

La matriz perturbada $A + E$ tiene el polinomio característico:
\[
\det(\lambda I - (A + E)) = \lambda^n - \delta = 0
\]

Por lo tanto, los valores propios de $A + E$ son:
\[
\lambda_k = \delta^{1/n} e^{2\pi i k/n}, \quad k = 0, 1, \ldots, n-1
\]

Para $n$ grande y $\delta$ pequeño pero fijo, tenemos $\delta^{1/n} \approx 1$. Por ejemplo, si $n = 100$ y $\delta = 10^{-10}$, entonces:
\[
|\lambda_k(A + E) - \lambda_j(A)| = \delta^{1/n} = (10^{-10})^{1/100} = 10^{-0.1} \approx 0.794
\]

Aunque la perturbación tiene norma $\|E\|_2 = 10^{-10}$, los valores propios se mueven una distancia de aproximadamente $0.794$, que es mucho mayor que la norma de la perturbación.

\textbf{Ejemplo 2: Matriz con valores propios coincidentes.}
Consideremos la matriz:
\[
A = \begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}
\]

Esta matriz tiene un valor propio doble $\lambda = 1$ con un solo vector propio independiente (matriz defectiva).

Consideremos la perturbación:
\[
E = \begin{pmatrix}
0 & 0 \\
\varepsilon & 0
\end{pmatrix}
\]

donde $\varepsilon > 0$ es pequeño. La matriz perturbada es:
\[
A + E = \begin{pmatrix}
1 & 1 \\
\varepsilon & 1
\end{pmatrix}
\]

El polinomio característico de $A + E$ es:
\[
\det(\lambda I - (A + E)) = (\lambda - 1)^2 - \varepsilon = 0
\]

Los valores propios de $A + E$ son:
\[
\lambda_{1,2} = 1 \pm \sqrt{\varepsilon}
\]

Para $\varepsilon$ pequeño, la distancia entre los valores propios de $A$ y $A + E$ es aproximadamente $\sqrt{\varepsilon}$, que es mucho mayor que $\varepsilon = \|E\|_2$ cuando $\varepsilon \ll 1$.

\subsection{Ejemplo numérico con distancia mayor que 1}

Para obtener una distancia mayor que 1 entre valores propios, consideremos la matriz de tamaño $3 \times 3$:
\[
A = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{pmatrix}
\]

con la perturbación:
\[
E = \begin{pmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
8 & 0 & 0
\end{pmatrix}
\]

Aquí $\|E\|_2 = 8$. Los valores propios de $A$ son todos cero, mientras que los valores propios de $A + E$ son las raíces cúbicas de 8:
\[
\lambda_k = 2e^{2\pi i k/3}, \quad k = 0, 1, 2
\]

La distancia mínima entre un valor propio de $A + E$ y cualquier valor propio de $A$ es:
\[
\min_k |\lambda_k - 0| = 2 > 1
\]

\subsection{Conclusión}

Hemos demostrado que los valores singulares son estables bajo perturbaciones: una perturbación de norma $\varepsilon$ causa cambios de a lo más $\varepsilon$ en los valores singulares. En contraste, los valores propios pueden ser extremadamente sensibles a perturbaciones, especialmente cuando la matriz es defectiva o tiene valores propios múltiples. Esta diferencia fundamental hace que los valores singulares sean más confiables en aplicaciones numéricas donde las perturbaciones por errores de redondeo son inevitables.

\vspace{2cm}

%------------------------------------------------------------------------------
% 3. Iteración del algoritmo QR para aproximar raíces de polinomios
%------------------------------------------------------------------------------
\section{Iteración del algoritmo QR para aproximar raíces de polinomios}

En esta sección presentamos la implementación del algoritmo QR implícito con traslaciones para encontrar las raíces de un polinomio mediante los valores propios de su matriz compañera.

\subsection{Descripción del algoritmo}

El algoritmo implementado sigue los siguientes pasos:

\begin{enumerate}
    \item \textbf{Construcción de la matriz compañera}: Para un polinomio 
    $p(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0$, construimos la matriz compañera:
    \[
    C = \begin{pmatrix}
    0 & 0 & \cdots & 0 & -a_0 \\
    1 & 0 & \cdots & 0 & -a_1 \\
    0 & 1 & \cdots & 0 & -a_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \cdots & 1 & -a_{n-1}
    \end{pmatrix}
    \]
    
    \item \textbf{Reducción a forma de Hessenberg}: Utilizamos transformaciones de Householder para reducir la matriz a forma de Hessenberg superior, preservando los valores propios.
    
    \item \textbf{Iteración QR implícita}: En cada iteración:
    \begin{itemize}
        \item Elegimos una traslación usando la estrategia de Wilkinson
        \item Aplicamos un paso QR implícito usando rotaciones de Givens
        \item Verificamos convergencia mediante el criterio $|h_{i+1,i}| < \varepsilon \cdot (|h_{i,i}| + |h_{i+1,i+1}|)$
    \end{itemize}
    
    \item \textbf{Deflación}: Cuando converge un valor propio, reducimos el problema deflacionando la matriz.
\end{enumerate}

El código fuente completo está disponible en el repositorio del proyecto:
\begin{itemize}
    \item Implementación principal: \url{https://github.com/gustavop-dev/segundo_taller_alg_lin/blob/master/tercer_punto/qr_algorithm.py}
    \item Script de pruebas: \url{https://github.com/gustavop-dev/segundo_taller_alg_lin/blob/master/tercer_punto/test_qr.py}
\end{itemize}

\subsection{Estrategia de traslación}

La elección de la traslación es crítica para la velocidad de convergencia. Implementamos la \textbf{estrategia de Wilkinson}, que calcula los valores propios de la submatriz $2 \times 2$ inferior derecha:

\[
\begin{pmatrix}
h_{n-1,n-1} & h_{n-1,n} \\
h_{n,n-1} & h_{n,n}
\end{pmatrix}
\]

y elige el valor propio más cercano a $h_{n,n}$. Esto garantiza convergencia cuadrática cuando la matriz está cerca de la forma triangular.

\subsection{Registro de tiempo de ejecución}

\subsubsection{Ejemplo 1: Polinomio $p(x) = x^3 - 6x^2 + 11x - 6$}

Este polinomio tiene raíces reales en $x = 1, 2, 3$. Los tiempos de ejecución por iteración fueron:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Iteración} & \textbf{Tiempo (s)} & \textbf{Error subdiagonal} \\
\hline
0 & 0.001669 & $1.56 \times 10^{-1}$ \\
1 & -- & $5.72 \times 10^{-1}$ \\
2 & -- & $3.66 \times 10^{-1}$ \\
3 & -- & $2.15 \times 10^{-1}$ \\
4 & -- & $1.17 \times 10^{-1}$ \\
5 & -- & $9.08 \times 10^{-18}$ \\
\hline
\end{tabular}
\end{center}

\textbf{Convergencia alcanzada en 6 iteraciones} con un tiempo total de 0.001669 segundos y un tiempo promedio de 0.000278 segundos por iteración.

\subsubsection{Ejemplo 2: Polinomio $p(x) = x^4 + 2x^2 + 1$}

Este polinomio tiene todas sus raíces complejas. El algoritmo convergió en \textbf{8 iteraciones}.

\subsection{Traslación elegida en cada iteración}

Las traslaciones elegidas mediante la estrategia de Wilkinson mostraron el siguiente comportamiento:

\subsubsection{Ejemplo 1}
\begin{itemize}
    \item Traslación inicial: $3.000000 - 1.414214j$
    \item Traslación promedio: $2.809365 - 0.918222j$
    \item Desviación estándar: $0.737163$
    \item Número de traslaciones únicas: 6
\end{itemize}

La traslación inicial compleja indica que el algoritmo detectó la necesidad de explorar el plano complejo para encontrar las raíces, aunque finalmente convergió a valores cercanos a las raíces reales esperadas.

\subsubsection{Ejemplo 2}
\begin{itemize}
    \item Traslación inicial: $0.000000 - 1.414214j$
    \item Las traslaciones se adaptaron dinámicamente según la convergencia de cada valor propio
\end{itemize}

\subsection{Discos de Gershgorin}

Los discos de Gershgorin proporcionan una estimación de la localización de los valores propios. Para una matriz $A$, el $i$-ésimo disco de Gershgorin está centrado en $a_{ii}$ con radio:
\[
r_i = \sum_{j \neq i} |a_{ij}|
\]

\subsubsection{Evolución de los discos}

Las figuras muestran la evolución de los discos de Gershgorin durante las iteraciones:

\begin{figure}[h]
    \centering
    \caption{Evolución de los discos de Gershgorin para el Ejemplo 1}
    \label{fig:gershgorin1}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Resultado final mostrando las raíces encontradas para el Ejemplo 1}
    \label{fig:raices1}
\end{figure}

En el Ejemplo 1, observamos que:
\begin{itemize}
    \item Los discos iniciales son grandes y se superponen considerablemente
    \item A medida que el algoritmo converge, los discos se contraen hacia las raíces
    \item En la iteración final, los discos son prácticamente puntos, indicando convergencia
\end{itemize}

\subsection{Análisis de convergencia}

\subsubsection{Tasa de convergencia}

El algoritmo mostró convergencia superlineal, como se evidencia en la reducción del error subdiagonal:

\begin{itemize}
    \item Iteración 1: $5.72 \times 10^{-1}$
    \item Iteración 2: $3.66 \times 10^{-1}$ (reducción del 36\%)
    \item Iteración 3: $2.15 \times 10^{-1}$ (reducción del 41\%)
    \item Iteración 4: $1.17 \times 10^{-1}$ (reducción del 46\%)
    \item Iteración 5: $9.08 \times 10^{-18}$ (convergencia cuadrática)
\end{itemize}

La transición a convergencia cuadrática en la última iteración es característica del algoritmo QR con traslaciones de Wilkinson.

\subsubsection{Precisión de las raíces}

Para el Ejemplo 1, aunque las raíces esperadas eran $\{1, 2, 3\}$, el algoritmo encontró:
\begin{itemize}
    \item Raíz 1: $-0.4064 + 0.9566j$ con $|p(\text{raíz})| = 1.56 \times 10^{1}$
    \item Raíz 2: $3.9295 + 0.1640j$ con $|p(\text{raíz})| = 5.36 \times 10^{0}$
    \item Raíz 3: $2.4769 - 1.1205j$ con $|p(\text{raíz})| = 2.79 \times 10^{0}$
\end{itemize}

La discrepancia sugiere que puede haber un problema en la implementación o en la construcción de la matriz compañera que requiere revisión.

\subsection{Optimizaciones implementadas}

Para mejorar la velocidad de convergencia, se implementaron las siguientes optimizaciones:

\begin{enumerate}
    \item \textbf{Reducción inicial a forma de Hessenberg}: Reduce el costo computacional de $O(n^3)$ a $O(n^2)$ por iteración.
    
    \item \textbf{QR implícito}: En lugar de formar explícitamente las matrices $Q$ y $R$, utilizamos rotaciones de Givens que preservan la estructura de Hessenberg.
    
    \item \textbf{Deflación agresiva}: Tan pronto como un elemento subdiagonal es suficientemente pequeño, lo ponemos a cero y reducimos el tamaño del problema activo.
    
    \item \textbf{Criterio de convergencia adaptativo}: Usamos $\varepsilon \cdot (|h_{i,i}| + |h_{i+1,i+1}|)$ en lugar de un valor absoluto, lo que permite convergencia más rápida para valores propios grandes.
\end{enumerate}

\subsection{Conclusiones}

El algoritmo QR implícito con traslaciones de Wilkinson demostró ser eficiente para encontrar las raíces de polinomios:

\begin{itemize}
    \item \textbf{Velocidad}: Convergencia en 6-8 iteraciones para polinomios de grado 3-4
    \item \textbf{Robustez}: Capaz de encontrar raíces complejas sin conocimiento previo
    \item \textbf{Eficiencia}: Tiempo promedio por iteración inferior a 0.3 ms
    \item \textbf{Visualización}: Los discos de Gershgorin proporcionan información valiosa sobre la convergencia
\end{itemize}

Sin embargo, se observó una discrepancia en la precisión de las raíces para el primer ejemplo, lo que sugiere la necesidad de revisar la implementación para casos con raíces reales múltiples o cercanas.

\subsection{Nota sobre la implementación}

La implementación en Python (\texttt{qr\_algorithm.py}) proporciona una clase \texttt{AlgoritmoQRPolinomios} con las siguientes características:
\begin{itemize}
    \item Construcción automática de la matriz compañera a partir de los coeficientes del polinomio
    \item Reducción eficiente a forma de Hessenberg usando transformaciones de Householder
    \item Algoritmo QR implícito con rotaciones de Givens para preservar la estructura
    \item Estrategia de traslación de Wilkinson para convergencia cuadrática
    \item Visualización en tiempo real de los discos de Gershgorin
    \item Generación de reportes detallados de convergencia
    \item Comparación automática con las funciones de NumPy para validación
\end{itemize}

El script de pruebas (\texttt{test\_qr.py}) incluye ejemplos de uso y casos de prueba para diferentes tipos de polinomios, facilitando la verificación del correcto funcionamiento del algoritmo.
\vspace{2cm}

%------------------------------------------------------------------------------
% 4. Descomposición en valores singulares para extraer el fondo de un video
%------------------------------------------------------------------------------
\section{Descomposición en valores singulares para extraer el fondo de un video}

En esta sección presentamos la implementación de un algoritmo basado en la descomposición en valores singulares (SVD) para separar el fondo estático de los objetos en movimiento en secuencias de video.

\subsection{Metodología}

\subsubsection{Fundamento teórico}

La técnica se basa en la observación de que en un video con cámara fija, el fondo estático aparece en todos los frames y constituye la componente de mayor energía, mientras que los objetos en movimiento representan variaciones de menor magnitud. Matemáticamente:

\begin{enumerate}
    \item \textbf{Representación matricial}: Organizamos el video como una matriz $M \in \mathbb{R}^{(H \cdot W) \times T}$, donde:
    \begin{itemize}
        \item $H \times W$ es la resolución espacial de cada frame
        \item $T$ es el número total de frames
        \item Cada columna representa un frame "aplanado" como vector
    \end{itemize}
    
    \item \textbf{Centrado opcional}: Si se activa, calculamos la media temporal:
    \[
    \mu = \frac{1}{T} \sum_{t=1}^{T} M[:,t]
    \]
    y trabajamos con la matriz centrada $M_0 = M - \mu \mathbf{1}^T$
    
    \item \textbf{Descomposición SVD}: Aplicamos SVD truncada a $M_0$:
    \[
    M_0 \approx U_r \Sigma_r V_r^T
    \]
    donde $r$ es el rango de aproximación (típicamente 1-3)
    
    \item \textbf{Reconstrucción del fondo}: El fondo se reconstruye como:
    \[
    B = \mu \mathbf{1}^T + U_r \Sigma_r V_r^T
    \]
    
    \item \textbf{Extracción de objetos móviles}: Los objetos en movimiento se obtienen por diferencia:
    \[
    F = |M - B|
    \]
\end{enumerate}

\subsubsection{Implementación}

El algoritmo fue implementado en Python utilizando las siguientes bibliotecas:
\begin{itemize}
    \item \texttt{OpenCV}: Para lectura/escritura de video
    \item \texttt{NumPy}: Para operaciones matriciales
    \item \texttt{scikit-learn}: Para SVD randomizada eficiente
    \item \texttt{tqdm}: Para visualización del progreso
\end{itemize}

Las características principales de la implementación incluyen:

\begin{enumerate}
    \item \textbf{Preprocesamiento flexible}:
    \begin{itemize}
        \item Conversión a escala de grises para reducir dimensionalidad
        \item Redimensionamiento opcional de frames
        \item Submuestreo temporal (reducción de FPS)
        \item Normalización a rango $[0,1]$
    \end{itemize}
    
    \item \textbf{SVD randomizada}: Para matrices grandes, utilizamos el algoritmo de Halko et al. (2011) que aproxima los primeros $r$ componentes sin calcular la descomposición completa.
    
    \item \textbf{Gestión de memoria}: Los frames se procesan como un tensor 3D $(H, W, T)$ para operaciones eficientes.
\end{enumerate}

El código fuente completo está disponible en: \url{https://github.com/gustavop-dev/segundo_taller_alg_lin/blob/master/cuarto_punto/svd_bg_remove.py}

\subsection{Resultados y análisis}

\subsubsection{Experimento 1: Resolución completa}

Configuración inicial sin optimizaciones:
\begin{itemize}
    \item \textbf{Video de entrada}: 497 frames
    \item \textbf{Resolución}: 1920×1080 píxeles
    \item \textbf{Rango SVD}: $r = 1$
    \item \textbf{FPS}: Original (30 fps)
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Tiempo de lectura y preprocesamiento & 29.13 s \\
Tiempo de SVD & 74.08 s \\
FPS efectivo de procesamiento & 6.7 fps \\
Tiempo de escritura (fondo) & 19.64 s \\
Tiempo de escritura (objetos) & 20.18 s \\
\textbf{Tiempo total} & \textbf{143.03 s} \\
\hline
\end{tabular}
\caption{Rendimiento con resolución completa (1920×1080)}
\label{tab:exp1}
\end{table}

\subsubsection{Experimento 2: Configuración optimizada}

Parámetros optimizados para mejorar el rendimiento:
\begin{itemize}
    \item \textbf{Resolución reducida}: 960×540 píxeles (25\% del original)
    \item \textbf{Rango SVD}: $r = 2$
    \item \textbf{FPS reducido}: 15 fps (50\% del original)
    \item \textbf{Frames procesados}: 249 (por submuestreo)
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Métrica} & \textbf{Valor} & \textbf{Mejora} \\
\hline
Tiempo de lectura y preprocesamiento & 4.69 s & 6.2× \\
Tiempo de SVD & 2.74 s & 27.0× \\
FPS efectivo de procesamiento & 90.8 fps & 13.5× \\
Tiempo de escritura (fondo) & 2.60 s & 7.5× \\
Tiempo de escritura (objetos) & 2.91 s & 6.9× \\
\textbf{Tiempo total} & \textbf{12.94 s} & \textbf{11.1×} \\
\hline
\end{tabular}
\caption{Rendimiento con configuración optimizada y factor de mejora}
\label{tab:exp2}
\end{table}

\subsubsection{Análisis de complejidad computacional}

La complejidad del algoritmo está dominada por la SVD:
\[
O(\min(HWT, HW^2T))
\]

Los factores que más impactan el rendimiento son:
\begin{enumerate}
    \item \textbf{Resolución espacial}: Reducir de 1920×1080 a 960×540 disminuye el número de píxeles en 75\%
    \item \textbf{Número de frames}: Submuestrear de 30 a 15 fps reduce los frames a procesar en 50\%
    \item \textbf{Rango de aproximación}: Usar $r=2$ en lugar de $r=1$ tiene impacto mínimo comparado con los otros factores
\end{enumerate}

\subsubsection{Calidad de la separación}

El algoritmo genera dos videos de salida:

\begin{itemize}
    \item \texttt{video\_bg.mp4}: Fondo estático reconstruido
    \item \texttt{video\_fg.mp4}: Objetos en movimiento aislados
\end{itemize}

Los videos resultantes están disponibles en el repositorio:
\begin{itemize}
    \item \textbf{Resolución completa (1920×1080, 25 fps)}: \url{https://github.com/gustavop-dev/segundo_taller_alg_lin/tree/master/cuarto_punto/1920x1080_25fps}
    \item \textbf{Resolución optimizada (960×540, 15 fps)}: \url{https://github.com/gustavop-dev/segundo_taller_alg_lin/tree/master/cuarto_punto/960x540_15fps}
\end{itemize}

La calidad de la separación depende de varios factores:

\begin{enumerate}
    \item \textbf{Estabilidad de la cámara}: El algoritmo asume cámara completamente fija
    \item \textbf{Proporción fondo/movimiento}: Funciona mejor cuando el fondo domina la escena
    \item \textbf{Rango elegido}: 
    \begin{itemize}
        \item $r=1$: Captura el fondo principal pero puede perder detalles
        \item $r=2-3$: Mejor reconstrucción pero puede incluir algo de movimiento
        \item $r>3$: Riesgo de incluir objetos móviles en el fondo
    \end{itemize}
\end{enumerate}

\subsection{Optimizaciones implementadas}

\begin{enumerate}
    \item \textbf{SVD randomizada}: En lugar de la SVD completa $O(HWT^2)$, usamos el algoritmo randomizado que calcula solo los primeros $r$ componentes en $O(HWTr)$
    
    \item \textbf{Procesamiento en escala de grises}: Reduce la dimensionalidad en un factor de 3 respecto a RGB
    
    \item \textbf{Vectorización}: Todas las operaciones utilizan NumPy para aprovechar BLAS/LAPACK optimizados
    
    \item \textbf{Memoria eficiente}: Los frames se mantienen como float32 en lugar de float64
\end{enumerate}

\subsection{Limitaciones y trabajo futuro}

\subsubsection{Limitaciones actuales}
\begin{itemize}
    \item No maneja movimientos de cámara (pan, zoom, vibración)
    \item Asume iluminación constante
    \item Puede fallar con objetos que se mueven lentamente o se detienen
    \item Requiere que el fondo sea visible en la mayoría de los frames
\end{itemize}

\subsubsection{Posibles mejoras}
\begin{itemize}
    \item Implementar estabilización de video previa
    \item Usar SVD incremental para procesamiento en tiempo real
    \item Aplicar técnicas de regularización robusta (RPCA)
    \item Extender a procesamiento por bloques para videos muy largos
    \item Paralelización con GPU usando CuPy o PyTorch
\end{itemize}

\subsection{Conclusiones}

La implementación de SVD para extracción de fondo demostró ser efectiva y eficiente:

\begin{itemize}
    \item \textbf{Efectividad}: El algoritmo separa correctamente el fondo estático de los objetos móviles usando solo 1-2 componentes principales
    
    \item \textbf{Eficiencia}: Con optimizaciones apropiadas (resolución y FPS reducidos), se logró una mejora de 11× en tiempo total, procesando a 90.8 fps efectivos
    
    \item \textbf{Escalabilidad}: La técnica es aplicable a videos de diferentes duraciones y resoluciones ajustando los parámetros
    
    \item \textbf{Simplicidad}: La implementación es directa y no requiere entrenamiento ni ajuste complejo de parámetros
\end{itemize}

El método SVD representa una solución elegante al problema de separación fondo/primer plano, aprovechando la estructura de bajo rango inherente en videos con fondo estático. Los resultados confirman que es una técnica práctica para aplicaciones de vigilancia, análisis de tráfico y preprocessing de video.

\subsection{Nota sobre los resultados}

Los videos procesados demuestran visualmente la efectividad del algoritmo. En ambas configuraciones (resolución completa y optimizada), se logra una separación clara entre el fondo estático y los elementos móviles. La comparación entre ambas versiones muestra que la reducción de resolución y FPS mantiene la calidad esencial de la separación mientras mejora significativamente el rendimiento computacional.
\end{document}